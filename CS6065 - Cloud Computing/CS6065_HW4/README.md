# CS6065_HW4

**Disclaimer - Since the grading of this assignment, the AWS Cloud Server hosting the following Public IP Address has been terminated.**

A simple MapReduce program utilizing Hadoop's streaming API in order to analyze data and yield summary counts for each vehicle type that was involved in an accident. The analyzed data was taken from the City of New York's database on vehicular incidents over a period of time.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites

* University of Cincinnati (UC) email address
* Enrollment in the College of Engineering and Applied Science (CEAS)
* Access to the UC Network via direct or remote access through the [UC Virtual Private Network (VPN) Client](https://sslvpn.uc.edu)
* Linux or Mac OS

### Running the tests

Open the CMD and use the following ssh-command in order to access the Hadoop Cluster, where username represents your UC 6+2. You will then be prompted to enter your UC password:
```
$ ssh username@hadoop-gate-0.eecs.uc.edu
```
Create a directory and set it as your working directory using the following commands:
```
$ mkdir working_file_name
$ cd working_file_name
```
Use the following command to clone this Git Repository into the working directory for testing:
```
$ git clone https://github.uc.edu/huangar/CS6065_HW4.git
```
Once the working directory has a copy of the Git Repository, use the following command in order to run the MapReduce job on the Hadoop Cluster, with output_data_location_on_hdfs representing the created pathname at which the output will be stored:
```
$ hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input /user/tatavag/nyc.data -output output_data_location_on_hdfs
```
Example output snippet of the previous command in the console:
```
[...] WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
packageJobJar: [mapper.py, reducer.py] [/usr/hdp/3.1.0.0-78/hadoop-mapreduce/hadoop-streaming-3.1.1.3.1.0.0-78.jar] /tmp/streamjob6073568192155626497.jar tmpDir=null
[...] INFO client.RMProxy: Connecting to ResourceManager at hdfs-0-3.eecscluster/192.168.200.103:8050
[...] INFO client.AHSProxy: Connecting to Application History server at hdfs-0-0.eecscluster/192.168.200.100:10200
[...] INFO client.RMProxy: Connecting to ResourceManager at hdfs-0-3.eecscluster/192.168.200.103:8050
[...] INFO client.AHSProxy: Connecting to Application History server at hdfs-0-0.eecscluster/192.168.200.100:10200
[...] INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/huangar/.staging/job_1549995810963_2627
[...] INFO mapred.FileInputFormat: Total input files to process : 1
[...] INFO mapreduce.JobSubmitter: number of splits:3
[...] INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1549995810963_2627
[...] INFO mapreduce.JobSubmitter: Executing with tokens: []
[...] INFO conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.0.0-78/0/resource-types.xml
[...] INFO impl.YarnClientImpl: Submitted application application_1549995810963_2627
[...] INFO mapreduce.Job: The url to track the job: http://hdfs-0-3.eecscluster:8088/proxy/application_1549995810963_2627/
[...] INFO mapreduce.Job: Running job: job_1549995810963_2627
[...] INFO mapreduce.Job: Job job_1549995810963_2627 running in uber mode : false
[...] INFO mapreduce.Job:  map 0% reduce 0%
[...] INFO mapreduce.Job:  map 43% reduce 0%
[...] INFO mapreduce.Job:  map 52% reduce 0%
[...] INFO mapreduce.Job:  map 59% reduce 0%
[...] INFO mapreduce.Job:  map 66% reduce 0%
[...] INFO mapreduce.Job:  map 83% reduce 0%
[...] INFO mapreduce.Job:  map 100% reduce 0%
[...] INFO mapreduce.Job:  map 100% reduce 68%
[...] INFO mapreduce.Job:  map 100% reduce 100%
[...] INFO mapreduce.Job: Job job_1549995810963_2627 completed successfully
```
In order to access the results generated by the job, find the pathname of the job output using the following command:
```
$ hadoop fs -ls /user/username/output_data_location_on_hdfs
```
Example output snippet of the previous command in the console, with job_results representing the results generated by the above job:
```
Found 2 items
-rw-r--r--   3 username hdfs          0 2019-04-23 18:23 /user/username/output_data_location_on_hdfs/_SUCCESS
-rw-r--r--   3 username hdfs       7123 2019-04-23 18:23 /user/username/output_data_location_on_hdfs/job_results
```
This result can be accessed using the following command:
```
$ hadoop fs -cat /user/huangar/output_data_location_on_hdfs/job_results
```
Example output snippet of the previous command in the console:
```
...
FIRE TRUCK	1509
FIRET	34
FLAT	20
FLATB	9
FLEET	1
FLTRL	1
FOOD	6
FORD	9
FORK	17
...
```
### Built with

* TextWrangler

### Author

* Andrew Huang

### Acknowledgments

* Michael Noll for his comprehensive write-up on the implementation of a Hadoop MapReduce in Python
